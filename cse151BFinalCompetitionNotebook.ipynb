{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11814693,"sourceType":"datasetVersion","datasetId":7420771}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üåé Welcome to the CSE151B Spring 2025 Climate Emulation Competition!\n\nThank you for participating in this exciting challenge focused on building machine learning models to emulate complex climate systems.  \nThis notebook is provided as a **starter template** to help yo`u:\n\n- Understand how to load and preprocess the dataset  \n- Construct a baseline model  \n- Train and evaluate predictions using a PyTorch Lightning pipeline  \n- Format your predictions for submission to the leaderboard  \n\nYou're encouraged to:\n- Build on this structure or replace it entirely\n- Try more advanced models and training strategies\n- Incorporate your own ideas to push the boundaries of what's possible\n\nIf you're interested in developing within a repository structure and/or use helpful tools like configuration management (based on Hydra) and logging (with Weights & Biases), we recommend checking out the following Github repo. Such a structure can be useful when running multiple experiments and trying various research ideas.\n\nüëâ [https://github.com/salvaRC/cse151b-spring2025-competition](https://github.com/salvaRC/cse151b-spring2025-competition)\n\nGood luck, have fun, and we hope you learn a lot through this process!\n","metadata":{}},{"cell_type":"markdown","source":"### üì¶ Install Required Libraries\nWe install the necessary Python packages for data loading, deep learning, and visualization.\n","metadata":{}},{"cell_type":"code","source":"!pip install xarray zarr dask lightning matplotlib wandb cftime einops --quiet\n\nimport os\nfrom datetime import datetime\nimport numpy as np\nimport xarray as xr\nimport dask.array as da\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nimport lightning.pytorch as pl\nfrom lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T02:11:20.138926Z","iopub.execute_input":"2025-06-01T02:11:20.139124Z","iopub.status.idle":"2025-06-01T02:11:39.995252Z","shell.execute_reply.started":"2025-06-01T02:11:20.139105Z","shell.execute_reply":"2025-06-01T02:11:39.994530Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ‚öôÔ∏è Configuration Setup  \nDefine all model, data, and training hyperparameters in one place for easy control and reproducibility.\n\n### üìä Data Configuration\n\nWe define the dataset settings used for training and evaluation. This includes:\n\n- **`path`**: Path to the `.zarr` dataset containing monthly climate variables from CMIP6 simulations.\n- **`input_vars`**: Climate forcing variables (e.g., CO‚ÇÇ, CH‚ÇÑ) used as model inputs.\n- **`output_vars`**: Target variables to predict ‚Äî surface air temperature (`tas`) and precipitation (`pr`).\n- **`target_member_id`**: Ensemble member to use from the simulations (each SSP has 3) for target variables.\n- **`train_ssps`**: SSP scenarios used for training (low to high emissions).\n- **`test_ssp`**: Scenario held out for evaluation (Must be set to SSP245).\n- **`test_months`**: Number of months to include in the test split (Must be set to 120).\n- **`batch_size`** and **`num_workers`**: Data loading parameters for PyTorch training.\n\nThese settings reflect how the challenge is structured: models must learn from some emission scenarios and generalize to unseen ones.\n\n> ‚ö†Ô∏è **Important:** Do **not modify** the following test settings:\n>\n> - `test_ssp` must remain **`ssp245`**, which is the held-out evaluation scenario.\n> - `test_months` must be **`120`**, corresponding to the last 10 years (monthly resolution) of the scenario.\n\n","metadata":{}},{"cell_type":"code","source":"#NOTE Change the data directory according to where you have your zarr files stored\nconfig = {\n    \"data\": {\n        \"path\": \"/kaggle/input/cse151b-spring2025-competition/processed_data_cse151b_v2_corrupted_ssp245/processed_data_cse151b_v2_corrupted_ssp245.zarr\",\n        \"input_vars\": [\"CO2\", \"SO2\", \"CH4\", \"BC\", \"rsdt\"],\n        \"output_vars\": [\"tas\", \"pr\"],\n        \"target_member_id\": 0,\n        \"train_ssps\": [\"ssp126\", \"ssp370\", \"ssp585\"],\n        \"test_ssp\": \"ssp245\",\n        \"test_months\": 360,\n        \"batch_size\": 16,\n        \"num_workers\": 4,\n    },\n    \"model\": {\n        \"type\": \"unetAttention\",\n        \"kernel_size\": 3,\n        \"init_dim\": 64,\n        \"depth\": 6,\n        \"dropout_rate\": 0.3,\n    },\n    \"training\": {\n        \"lr\": 5e-5,\n    },\n    \"trainer\": {\n        \"max_epochs\": 30,\n        \"accelerator\": \"auto\",\n        \"devices\": \"auto\",\n        \"precision\": 16,\n        \"deterministic\": True,\n        \"num_sanity_val_steps\": 0,\n    },\n    \"seed\": 42,\n}\npl.seed_everything(config[\"seed\"])  # Set seed for reproducibility","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T02:11:39.996030Z","iopub.execute_input":"2025-06-01T02:11:39.996502Z","iopub.status.idle":"2025-06-01T02:11:40.009569Z","shell.execute_reply.started":"2025-06-01T02:11:39.996480Z","shell.execute_reply":"2025-06-01T02:11:40.008715Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### üîß Spatial Weighting Utility Function\n\nThis cell sets up utility functions for reproducibility and spatial weighting:\n\n- **`get_lat_weights(latitude_values)`**: Computes cosine-based area weights for each latitude, accounting for the Earth's curvature. This is critical for evaluating global climate metrics fairly ‚Äî grid cells near the equator represent larger surface areas than those near the poles.\n","metadata":{}},{"cell_type":"code","source":"def get_lat_weights(latitude_values):\n    lat_rad = np.deg2rad(latitude_values)\n    weights = np.cos(lat_rad)\n    return weights / np.mean(weights)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T02:11:40.010424Z","iopub.execute_input":"2025-06-01T02:11:40.010688Z","iopub.status.idle":"2025-06-01T02:11:40.025198Z","shell.execute_reply.started":"2025-06-01T02:11:40.010667Z","shell.execute_reply":"2025-06-01T02:11:40.024604Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### üß† SimpleCNN: A Residual Convolutional Baseline\n\nThis is a lightweight baseline model designed to capture spatial patterns in global climate data using convolutional layers.\n\n- The architecture starts with a **convolution + batch norm + ReLU** block to process the input channels.\n- It then applies a series of **residual blocks** to extract increasingly abstract spatial features. These help preserve gradient flow during training.\n- Finally, a few convolutional layers reduce the feature maps down to the desired number of output channels (`tas` and `pr`).\n\nThis model only serves as a **simple baseline for climate emulation**. \n\nWe encourage you to build and experiment with your own models and ideas.\n","metadata":{}},{"cell_type":"code","source":"# Define a Residual Block which is a building block for ResNet-like architectures\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1):\n        super().__init__()\n        # First convolution layer with BatchNorm and ReLU activation\n        # Padding is set to keep spatial dimensions same (kernel_size // 2)\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=kernel_size // 2)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        \n        # Second convolution layer followed by BatchNorm (no activation yet)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size, padding=kernel_size // 2)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        \n        # Skip connection (identity or projection) to match output dimensions\n        self.skip = nn.Sequential()\n        # If input and output dimensions or stride differ, adjust skip connection via 1x1 conv\n        if stride != 1 or in_channels != out_channels:\n            self.skip = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n                nn.BatchNorm2d(out_channels)\n            )\n\n    def forward(self, x):\n        identity = x  # Save input to add back later (skip connection)\n        \n        # Forward pass through first conv, batch norm, and ReLU\n        out = self.relu(self.bn1(self.conv1(x)))\n        \n        # Forward pass through second conv and batch norm (no activation yet)\n        out = self.bn2(self.conv2(out))\n        \n        # Add skip connection (either identity or adjusted input)\n        out += self.skip(identity)\n        \n        # Final activation after addition\n        return self.relu(out)\n\n# Define a simple CNN model using ResidualBlocks\nclass SimpleCNN(nn.Module):\n    def __init__(self, n_input_channels, n_output_channels, kernel_size=3, init_dim=64, depth=4, dropout_rate=0.2):\n        super().__init__()\n        \n        # Initial convolution block: Conv + BatchNorm + ReLU\n        # Converts input channels to init_dim feature maps\n        self.initial = nn.Sequential(\n            nn.Conv2d(n_input_channels, init_dim, kernel_size=kernel_size, padding=kernel_size // 2),\n            nn.BatchNorm2d(init_dim),\n            nn.ReLU(inplace=True),\n        )\n        \n        # List to hold ResidualBlocks\n        self.res_blocks = nn.ModuleList()\n        current_dim = init_dim\n        \n        # Create 'depth' number of ResidualBlocks\n        for i in range(depth):\n            # Double the number of channels for each block except the last one\n            out_dim = current_dim * 2 if i < depth - 1 else current_dim\n            self.res_blocks.append(ResidualBlock(current_dim, out_dim))\n            if i < depth - 1:\n                current_dim *= 2  # Update current_dim after doubling\n        \n        # Dropout layer for regularization, applied after residual blocks\n        self.dropout = nn.Dropout2d(dropout_rate)\n        \n        # Final convolutional layers to reduce channels to output channels\n        # Includes Conv + BatchNorm + ReLU followed by a 1x1 Conv to output channels\n        self.final = nn.Sequential(\n            nn.Conv2d(current_dim, current_dim // 2, kernel_size=kernel_size, padding=kernel_size // 2),\n            nn.BatchNorm2d(current_dim // 2),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(current_dim // 2, n_output_channels, kernel_size=1),\n        )\n\n    def forward(self, x):\n        # Pass input through initial block\n        x = self.initial(x)\n        \n        # Pass through each residual block sequentially\n        for res_block in self.res_blocks:\n            x = res_block(x)\n        \n        # Apply dropout and then final layers to get output\n        return self.final(self.dropout(x))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T02:11:40.025952Z","iopub.execute_input":"2025-06-01T02:11:40.026123Z","iopub.status.idle":"2025-06-01T02:11:40.039361Z","shell.execute_reply.started":"2025-06-01T02:11:40.026108Z","shell.execute_reply":"2025-06-01T02:11:40.038722Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# A convolutional block used in the UNet encoder and decoder paths\nclass UNetConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3, dropout=0.1):\n        super().__init__()\n        # First convolution layer + ReLU activation\n        # Padding is set to maintain spatial size\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, padding=kernel_size // 2)\n        self.relu1 = nn.ReLU(inplace=True)\n        \n        # Dropout layer for regularization after first conv+relu\n        self.drop = nn.Dropout2d(dropout)\n        \n        # Second convolution layer + ReLU activation\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size, padding=kernel_size // 2)\n        self.relu2 = nn.ReLU(inplace=True)\n\n        # Skip connection to match input to output channels if needed\n        # Uses 1x1 convolution if channels differ, otherwise identity mapping\n        self.skip = nn.Conv2d(in_channels, out_channels, 1) if in_channels != out_channels else nn.Identity()\n\n    def forward(self, x):\n        identity = self.skip(x)  # Prepare skip connection (either conv or identity)\n        \n        out = self.relu1(self.conv1(x))  # First conv + relu\n        out = self.drop(out)              # Apply dropout\n        out = self.conv2(out)             # Second conv (no activation yet)\n        \n        # Add skip connection and apply final activation\n        return self.relu2(out + identity)\n\n# UNet architecture implementing encoder-decoder with skip connections\nclass UNet(nn.Module):\n    def __init__(\n        self,\n        n_input_channels,\n        n_output_channels,\n        kernel_size=3,\n        init_dim=64,\n        dropout_rate=0.1,\n        depth=4,\n    ):\n        super().__init__()\n        self.depth = depth\n        \n        # Compute number of channels for each level of the UNet (doubling every step)\n        self.dims = [init_dim * (2 ** i) for i in range(depth + 1)]\n\n        # Encoder blocks and downsampling layers\n        self.encoders = nn.ModuleList()\n        self.downsamples = nn.ModuleList()\n        in_ch = n_input_channels\n        \n        for i in range(depth):\n            # Encoder convolution block for current depth level\n            self.encoders.append(UNetConvBlock(in_ch, self.dims[i], kernel_size, dropout_rate))\n            \n            # Downsampling by stride 2 conv to reduce spatial dimensions by half\n            self.downsamples.append(\n                nn.Conv2d(self.dims[i], self.dims[i], kernel_size=3, stride=2, padding=1)\n            )\n            \n            in_ch = self.dims[i]  # Update input channels for next block\n\n        # Bottleneck block at the bottom of the UNet\n        self.bottleneck = UNetConvBlock(self.dims[depth - 1], self.dims[depth], kernel_size, dropout_rate)\n\n        # Decoder upsampling and convolution blocks\n        self.upconvs = nn.ModuleList()\n        self.decoders = nn.ModuleList()\n        \n        # Build decoder layers in reverse order\n        for i in reversed(range(depth)):\n            # Transpose convolution to upsample (double spatial size)\n            self.upconvs.append(\n                nn.ConvTranspose2d(self.dims[i + 1], self.dims[i], kernel_size=2, stride=2)\n            )\n            \n            # Decoder conv block takes concatenated features (upsampled + skip connection)\n            self.decoders.append(\n                UNetConvBlock(self.dims[i] * 2, self.dims[i], kernel_size, dropout_rate)\n            )\n\n        # Final conv layer to get desired number of output channels (e.g., segmentation classes)\n        self.final = nn.Conv2d(self.dims[0], n_output_channels, kernel_size=1)\n\n    def forward(self, x):\n        enc_outputs = []  # To store outputs for skip connections\n\n        # Encoder path: conv blocks + downsampling\n        for i in range(self.depth):\n            x = self.encoders[i](x)      # Apply encoder conv block\n            enc_outputs.append(x)        # Save output for skip connection\n            \n            x = self.downsamples[i](x)   # Downsample (reduce spatial size)\n\n        # Bottleneck conv block\n        x = self.bottleneck(x)\n\n        # Decoder path: upsampling + concatenation with skip features + conv blocks\n        for i in range(self.depth):\n            x = self.upconvs[i](x)  # Upsample feature map\n            \n            # Get corresponding encoder output for skip connection\n            enc = enc_outputs[self.depth - 1 - i]\n\n            # Handle any size mismatches due to odd dimensions by interpolation\n            if x.shape[-2:] != enc.shape[-2:]:\n                x = F.interpolate(x, size=enc.shape[-2:], mode='bilinear', align_corners=False)\n\n            # Concatenate along channel dimension\n            x = torch.cat([x, enc], dim=1)\n            \n            # Apply decoder conv block on concatenated features\n            x = self.decoders[i](x)\n\n        # Final conv layer to get output predictions\n        return self.final(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T02:11:40.040151Z","iopub.execute_input":"2025-06-01T02:11:40.040420Z","iopub.status.idle":"2025-06-01T02:11:40.060786Z","shell.execute_reply.started":"2025-06-01T02:11:40.040389Z","shell.execute_reply":"2025-06-01T02:11:40.059906Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Attention block used in UNet to modulate skip connections\nclass UNetAttentionBlock(nn.Module):\n    def __init__(self, F_g, F_l, F_int):\n        super().__init__()\n        # Transform gating signal (decoder features) to intermediate channels\n        self.W_g = nn.Sequential(\n            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0),\n            nn.BatchNorm2d(F_int)\n        )\n        # Transform skip connection features (encoder features) to intermediate channels\n        self.W_x = nn.Sequential(\n            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0),\n            nn.BatchNorm2d(F_int)\n        )\n        # Compute attention coefficients (single channel, sigmoid activation)\n        self.psi = nn.Sequential(\n            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0),\n            nn.Sigmoid()\n        )\n\n    def forward(self, g, x):\n        # Apply transformations\n        g1 = self.W_g(g)  # gating signal from decoder\n        x1 = self.W_x(x)  # features from encoder skip connection\n        \n        # Sum and apply non-linearity then sigmoid attention mask\n        psi = self.psi(F.relu(g1 + x1))\n        \n        # Scale encoder features by attention coefficients\n        return x * psi\n\n\n# Convolutional block with residual skip connection, used in attention UNet\nclass UNetAttentionConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3):\n        super().__init__()\n        # First conv + batch norm + ReLU\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, padding=kernel_size // 2)\n        self.norm1 = nn.BatchNorm2d(out_channels)\n        self.relu1 = nn.ReLU(inplace=True)\n\n        # Second conv + batch norm (no activation yet)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size, padding=kernel_size // 2)\n        self.norm2 = nn.BatchNorm2d(out_channels)\n        self.relu2 = nn.ReLU(inplace=True)\n\n        # Skip connection to match dimensions if needed\n        self.skip = nn.Conv2d(in_channels, out_channels, 1) if in_channels != out_channels else nn.Identity()\n\n    def forward(self, x):\n        identity = self.skip(x)  # Prepare skip connection\n\n        # Forward through conv layers and activations\n        out = self.relu1(self.norm1(self.conv1(x)))\n        out = self.norm2(self.conv2(out))\n        \n        # Add skip connection and apply final activation\n        return self.relu2(out + identity)\n\n\n# UNet architecture with attention gates on skip connections\nclass UNetAttention(nn.Module):\n    def __init__(self, n_input_channels, n_output_channels, kernel_size=3, init_dim=64, depth=4):\n        super().__init__()\n        self.depth = depth\n        \n        # Channel sizes at each depth level (doubling at each step)\n        self.dims = [init_dim * (2 ** i) for i in range(depth + 1)]\n\n        # Encoder blocks and downsampling layers\n        self.encoders = nn.ModuleList()\n        self.downsamples = nn.ModuleList()\n        in_ch = n_input_channels\n        \n        for i in range(depth):\n            # Encoder conv block\n            self.encoders.append(UNetAttentionConvBlock(in_ch, self.dims[i], kernel_size))\n            # Downsample spatial resolution by factor of 2\n            self.downsamples.append(\n                nn.Conv2d(self.dims[i], self.dims[i], kernel_size=3, stride=2, padding=1)\n            )\n            in_ch = self.dims[i]\n\n        # Bottleneck conv block at bottom of UNet\n        self.bottleneck = UNetAttentionConvBlock(self.dims[depth - 1], self.dims[depth], kernel_size)\n\n        # Decoder upconvs, attention blocks, and conv blocks\n        self.upconvs = nn.ModuleList()\n        self.attention_blocks = nn.ModuleList()\n        self.decoders = nn.ModuleList()\n        \n        # Build decoder path in reverse order\n        for i in reversed(range(depth)):\n            # Upsample by transpose convolution (double spatial size)\n            self.upconvs.append(\n                nn.ConvTranspose2d(self.dims[i + 1], self.dims[i], kernel_size=2, stride=2)\n            )\n            # Attention block modulates encoder skip features based on decoder features\n            self.attention_blocks.append(\n                UNetAttentionBlock(F_g=self.dims[i], F_l=self.dims[i], F_int=self.dims[i] // 2)\n            )\n            # Decoder conv block processes concatenated upsampled + attended skip features\n            self.decoders.append(UNetAttentionConvBlock(self.dims[i] * 2, self.dims[i], kernel_size))\n\n        # Final conv to produce output channels (e.g., segmentation classes)\n        self.final = nn.Conv2d(self.dims[0], n_output_channels, kernel_size=1)\n\n    def forward(self, x):\n        enc_outputs = []\n\n        # Encoder path\n        for i in range(self.depth):\n            x = self.encoders[i](x)\n            enc_outputs.append(x)    # Save encoder outputs for skip connections\n            x = self.downsamples[i](x)\n\n        # Bottleneck\n        x = self.bottleneck(x)\n\n        # Decoder path with attention gating on skip connections\n        for i in range(self.depth):\n            x = self.upconvs[i](x)  # Upsample\n\n            # Get corresponding encoder output\n            enc = enc_outputs[self.depth - 1 - i]\n\n            # Fix any spatial size mismatches due to rounding in down/up sampling\n            if x.shape[-2:] != enc.shape[-2:]:\n                x = F.interpolate(x, size=enc.shape[-2:], mode='bilinear', align_corners=False)\n\n            # Apply attention block to encoder skip features, conditioned on decoder features\n            enc = self.attention_blocks[i](x, enc)\n\n            # Concatenate upsampled decoder features with attended skip connection\n            x = torch.cat([x, enc], dim=1)\n\n            # Decode concatenated features\n            x = self.decoders[i](x)\n\n        # Final output conv layer\n        return self.final(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T02:11:40.062777Z","iopub.execute_input":"2025-06-01T02:11:40.062971Z","iopub.status.idle":"2025-06-01T02:11:40.082093Z","shell.execute_reply.started":"2025-06-01T02:11:40.062954Z","shell.execute_reply":"2025-06-01T02:11:40.081297Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Basic convolutional block used in ResNet, with residual connections and dropout\nclass ResNetConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3, dropout=0.1, stride=1, downsample=None):\n        super().__init__()\n        # First convolution layer with possible stride (for downsampling)\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=kernel_size // 2)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.drop = nn.Dropout2d(dropout)  # Spatial dropout for regularization\n        \n        # Second convolution layer (no stride here, same spatial size)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size, padding=kernel_size // 2)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        \n        # Optional downsample layer to match identity dimensions (for skip connection)\n        self.downsample = downsample\n\n    def forward(self, x):\n        identity = x  # Save input for skip connection\n\n        # Forward pass through first conv + batch norm + relu\n        out = self.relu(self.bn1(self.conv1(x)))\n        out = self.drop(out)  # Apply dropout\n        out = self.bn2(self.conv2(out))  # Second conv + batch norm (no activation yet)\n\n        # If downsample layer exists, apply to identity to match shapes\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        # Add skip connection (residual)\n        out += identity\n        return self.relu(out)  # Final activation\n\n\n# ResNet model with encoder and decoder parts\nclass ResNet(nn.Module):\n    def __init__(\n        self,\n        n_input_channels,\n        n_output_channels,\n        kernel_size=3,\n        init_dim=64,\n        dropout_rate=0.1,\n        depth=4,\n    ):\n        super().__init__()\n\n        # Define number of residual blocks in each layer based on depth\n        if depth == 4:\n            layers = [2, 2, 2, 2]       # ResNet-18 style\n        elif depth == 5:\n            layers = [3, 4, 6, 3]       # ResNet-34 style\n        elif depth == 6:\n            layers = [3, 4, 23, 3]      # ResNet-101 style\n        else:\n            raise ValueError(\"Unsupported depth\")\n\n        self.inplanes = init_dim  # Number of channels input to each block\n\n        # Initial convolution and batch norm + ReLU before ResNet layers\n        self.initial = nn.Sequential(\n            nn.Conv2d(n_input_channels, init_dim, kernel_size=7, stride=2, padding=3),\n            nn.BatchNorm2d(init_dim),\n            nn.ReLU(inplace=True),\n        )\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)  # Downsample spatial size\n\n        # Residual layers forming the encoder\n        self.layer0 = self._make_layer(\n            ResNetConvBlock, init_dim, layers[0], stride=1, kernel_size=kernel_size, dropout=dropout_rate\n        )\n        self.layer1 = self._make_layer(\n            ResNetConvBlock, init_dim * 2, layers[1], stride=2, kernel_size=kernel_size, dropout=dropout_rate\n        )\n        self.layer2 = self._make_layer(\n            ResNetConvBlock, init_dim * 4, layers[2], stride=2, kernel_size=kernel_size, dropout=dropout_rate\n        )\n        self.layer3 = self._make_layer(\n            ResNetConvBlock, init_dim * 8, layers[3], stride=2, kernel_size=kernel_size, dropout=dropout_rate\n        )\n\n        # Decoder blocks: transposed convolutions to upsample feature maps back to input size\n        self.upsample1 = self._upsample_block(init_dim * 8, init_dim * 4)\n        self.upsample2 = self._upsample_block(init_dim * 4, init_dim * 2)\n        self.upsample3 = self._upsample_block(init_dim * 2, init_dim)\n        self.upsample4 = self._upsample_block(init_dim, init_dim // 2)\n\n        # Final layers to reduce channels and produce output (e.g., segmentation classes)\n        self.final = nn.Sequential(\n            nn.Conv2d(init_dim // 2, init_dim // 2, kernel_size=3, padding=1),\n            nn.BatchNorm2d(init_dim // 2),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(init_dim // 2, n_output_channels, kernel_size=1),\n        )\n\n    # Helper function to create a sequence of residual blocks\n    def _make_layer(self, block, planes, blocks, stride=1, kernel_size=3, dropout=0.1):\n        downsample = None\n        # Create downsampling skip connection if dimensions or stride differ\n        if stride != 1 or self.inplanes != planes:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes),\n            )\n\n        # First block in the layer might downsample input\n        layers = [block(self.inplanes, planes, stride=stride, downsample=downsample, kernel_size=kernel_size, dropout=dropout)]\n        self.inplanes = planes  # Update inplanes for subsequent blocks\n\n        # Remaining blocks, no downsampling\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes, kernel_size=kernel_size, dropout=dropout))\n\n        return nn.Sequential(*layers)\n\n    # Helper function to create upsampling blocks using ConvTranspose2d\n    def _upsample_block(self, in_channels, out_channels):\n        return nn.Sequential(\n            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        input_size = x.shape[-2:]  # Save original spatial size for final interpolation\n\n        # Encoder path\n        x = self.initial(x)       # Initial conv layer\n        x = self.maxpool(x)       # Downsample\n\n        x = self.layer0(x)        # Residual layers with downsampling as configured\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n\n        # Decoder path: upsample step by step to increase spatial resolution\n        x = self.upsample1(x)\n        x = self.upsample2(x)\n        x = self.upsample3(x)\n        x = self.upsample4(x)\n\n        # Resize to original input size to ensure consistent output dimensions\n        x = F.interpolate(x, size=input_size, mode=\"bilinear\", align_corners=False)\n\n        # Final conv layers to produce output (e.g. segmentation map)\n        return self.final(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T02:11:40.083682Z","iopub.execute_input":"2025-06-01T02:11:40.083909Z","iopub.status.idle":"2025-06-01T02:11:40.102635Z","shell.execute_reply.started":"2025-06-01T02:11:40.083891Z","shell.execute_reply":"2025-06-01T02:11:40.101787Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Single dense block: BN -> ReLU -> Conv -> Dropout -> Concatenate input with output\nclass DenseBlock(nn.Module):\n    def __init__(self, in_channels, growth_rate, dropout=0.1):\n        super().__init__()\n        self.bn = nn.BatchNorm2d(in_channels)\n        self.relu = nn.ReLU(inplace=True)\n        # Conv layer outputs growth_rate feature maps\n        self.conv = nn.Conv2d(in_channels, growth_rate, kernel_size=3, padding=1, bias=False)\n        self.drop = nn.Dropout2d(dropout)\n\n    def forward(self, x):\n        # Apply BN -> ReLU -> Conv -> Dropout\n        out = self.conv(self.relu(self.bn(x)))\n        out = self.drop(out)\n        # Concatenate input and output along channel dimension (dense connectivity)\n        return torch.cat([x, out], dim=1)\n\n# Multiple DenseBlocks stacked sequentially, input channels grow with each block\nclass DenseLayer(nn.Module):\n    def __init__(self, num_layers, in_channels, growth_rate, dropout=0.1):\n        super().__init__()\n        layers = []\n        for i in range(num_layers):\n            # Each DenseBlock receives channels increased by growth_rate * number of previous blocks\n            layers.append(DenseBlock(in_channels + i * growth_rate, growth_rate, dropout))\n        self.block = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.block(x)\n\n# Transition layer reduces spatial dimensions and channels via BN, ReLU, 1x1 Conv, Dropout, and AvgPool\nclass TransitionLayer(nn.Module):\n    def __init__(self, in_channels, out_channels, dropout=0.1):\n        super().__init__()\n        self.bn = nn.BatchNorm2d(in_channels)\n        self.relu = nn.ReLU(inplace=True)\n        # 1x1 conv reduces channel count from in_channels to out_channels\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n        self.drop = nn.Dropout2d(dropout)\n        # Avg pooling reduces spatial dimensions by 2\n        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n\n    def forward(self, x):\n        x = self.relu(self.bn(x))\n        x = self.conv(x)\n        x = self.drop(x)\n        return self.pool(x)\n\n# DenseNet architecture combining dense blocks and transition layers, with an upsampling decoder\nclass DenseNet(nn.Module):\n    def __init__(\n        self,\n        n_input_channels,\n        n_output_channels,\n        kernel_size=3,\n        init_dim=64,\n        dropout_rate=0.1,\n        depth=4,\n        growth_rate=32,\n    ):\n        super().__init__()\n\n        # Define number of layers per dense block based on depth\n        if depth == 3:\n            layers = [6, 12, 24, 16]   # DenseNet-121 style\n        elif depth == 4:\n            layers = [6, 12, 32, 32]   # DenseNet-169 style\n        elif depth == 5:\n            layers = [6, 12, 48, 32]   # DenseNet-201 style\n        elif depth == 6:\n            layers = [6, 12, 64, 48]   # DenseNet-264 style\n        else:\n            raise ValueError(\"Unsupported depth\")\n\n        self.inplanes = init_dim       # Current number of channels, updated after each block\n        self.growth_rate = growth_rate # Number of channels added per DenseBlock\n        self.dropout_rate = dropout_rate\n\n        # Initial convolution layer to process input image\n        self.initial = nn.Sequential(\n            nn.Conv2d(n_input_channels, init_dim, kernel_size=7, stride=2, padding=3, bias=False),\n            nn.BatchNorm2d(init_dim),\n            nn.ReLU(inplace=True),\n        )\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)  # Downsample spatially\n\n        # Encoder: Dense blocks separated by transition layers (except after last block)\n        self.layer0 = self._make_layer(layers[0])\n        self.trans0 = self._make_transition()\n\n        self.layer1 = self._make_layer(layers[1])\n        self.trans1 = self._make_transition()\n\n        self.layer2 = self._make_layer(layers[2])\n        self.trans2 = self._make_transition()\n\n        self.layer3 = self._make_layer(layers[3])  # No transition after final dense block\n\n        # Decoder: upsample progressively to recover spatial resolution\n        self.upsample1 = self._upsample_block(self.inplanes, self.inplanes // 2)\n        self.upsample2 = self._upsample_block(self.inplanes // 2, self.inplanes // 4)\n        self.upsample3 = self._upsample_block(self.inplanes // 4, self.inplanes // 8)\n        self.upsample4 = self._upsample_block(self.inplanes // 8, self.inplanes // 16)\n\n        # Final conv layers to produce output with desired number of channels\n        self.final = nn.Sequential(\n            nn.Conv2d(self.inplanes // 16, self.inplanes // 16, kernel_size=kernel_size, padding=kernel_size // 2),\n            nn.BatchNorm2d(self.inplanes // 16),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(self.inplanes // 16, n_output_channels, kernel_size=1),\n        )\n\n    # Create a dense layer with multiple DenseBlocks; update current channels accordingly\n    def _make_layer(self, num_layers):\n        block = DenseLayer(num_layers, self.inplanes, self.growth_rate, self.dropout_rate)\n        self.inplanes += num_layers * self.growth_rate  # Channels increase by growth_rate * num_layers\n        return block\n\n    # Create a transition layer to reduce channels and downsample spatially by 2\n    def _make_transition(self):\n        out_channels = self.inplanes // 2  # Reduce channels by half\n        trans = TransitionLayer(self.inplanes, out_channels, self.dropout_rate)\n        self.inplanes = out_channels\n        return trans\n\n    # Upsampling block using transposed convolution followed by batch norm and ReLU\n    def _upsample_block(self, in_channels, out_channels):\n        return nn.Sequential(\n            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        input_size = x.shape[-2:]  # Save input spatial dimensions\n\n        x = self.initial(x)        # Initial conv + BN + ReLU\n        x = self.maxpool(x)        # Downsample spatially\n\n        # Encoder path: dense blocks separated by transitions\n        x = self.trans0(self.layer0(x))\n        x = self.trans1(self.layer1(x))\n        x = self.trans2(self.layer2(x))\n        x = self.layer3(x)  # Last block without transition\n\n        # Decoder path: upsample step-by-step to recover spatial resolution\n        x = self.upsample1(x)\n        x = self.upsample2(x)\n        x = self.upsample3(x)\n        x = self.upsample4(x)\n\n        # Interpolate to match original input size precisely\n        x = F.interpolate(x, size=input_size, mode=\"bilinear\", align_corners=False)\n\n        # Final conv layers to generate output (e.g., segmentation map)\n        return self.final(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T02:11:40.103408Z","iopub.execute_input":"2025-06-01T02:11:40.103727Z","iopub.status.idle":"2025-06-01T02:11:40.119610Z","shell.execute_reply.started":"2025-06-01T02:11:40.103696Z","shell.execute_reply":"2025-06-01T02:11:40.118801Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### üìê Normalizer: Z-Score Scaling for Climate Inputs & Outputs\n\nThis class handles **Z-score normalization**, a crucial preprocessing step for stable and efficient neural network training:\n\n- **`set_input_statistics(mean, std)` / `set_output_statistics(...)`**: Store the mean and standard deviation computed from the training data for later use.\n- **`normalize(data, data_type)`**: Standardizes the data using `(x - mean) / std`. This is applied separately to inputs and outputs.\n- **`inverse_transform_output(data)`**: Converts model predictions back to the original physical units (e.g., Kelvin for temperature, mm/day for precipitation).\n\nNormalizing the data ensures the model sees inputs with similar dynamic ranges and avoids biases caused by different variable scales.\n","metadata":{}},{"cell_type":"code","source":"class Normalizer:\n    def __init__(self):\n        self.mean_in, self.std_in = None, None\n        self.mean_out, self.std_out = None, None\n\n    def set_input_statistics(self, mean, std):\n        self.mean_in = mean\n        self.std_in = std\n\n    def set_output_statistics(self, mean, std):\n        self.mean_out = mean\n        self.std_out = std\n\n    def normalize(self, data, data_type):\n        if data_type == \"input\":\n            return (data - self.mean_in) / self.std_in\n        elif data_type == \"output\":\n            return (data - self.mean_out) / self.std_out\n\n    def inverse_transform_output(self, data):\n        return data * self.std_out + self.mean_out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T02:11:40.120345Z","iopub.execute_input":"2025-06-01T02:11:40.120665Z","iopub.status.idle":"2025-06-01T02:11:40.138882Z","shell.execute_reply.started":"2025-06-01T02:11:40.120646Z","shell.execute_reply":"2025-06-01T02:11:40.138137Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ClimateDataset(Dataset):\n    def __init__(self, inputs_dask, outputs_dask, output_is_normalized=True):\n        self.size = inputs_dask.shape[0]\n        print(f\"Creating dataset with {self.size} samples...\")\n\n        inputs_np = inputs_dask.compute()\n        outputs_np = outputs_dask.compute()\n\n        self.inputs = torch.from_numpy(inputs_np).float()\n        self.outputs = torch.from_numpy(outputs_np).float()\n\n        if torch.isnan(self.inputs).any() or torch.isnan(self.outputs).any():\n            raise ValueError(\"NaNs found in dataset\")\n\n    def __len__(self):\n        return self.size\n\n    def __getitem__(self, idx):\n        return self.inputs[idx], self.outputs[idx]\n\n\nclass ClimateDataModule(pl.LightningDataModule):\n    def __init__(\n        self,\n        path,\n        input_vars,\n        output_vars,\n        train_ssps,\n        test_ssp,\n        target_member_id,\n        val_months=120,\n        test_months=360,\n        batch_size=32,\n        num_workers=0,\n        seed=42,\n    ):\n        super().__init__()\n        self.path = path\n        self.input_vars = input_vars\n        self.output_vars = output_vars\n        self.train_ssps = train_ssps\n        self.test_ssp = test_ssp\n        self.target_member_id = target_member_id\n        self.val_months = val_months\n        self.test_months = test_months\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.seed = seed\n        self.normalizer = Normalizer()\n\n    def prepare_data(self):\n        assert os.path.exists(self.path), f\"Data path not found: {self.path}\"\n\n    def setup(self, stage=None):\n        ds = xr.open_zarr(self.path, consolidated=False, chunks={\"time\": 24})\n        spatial_template = ds[\"rsdt\"].isel(time=0, ssp=0, drop=True)\n\n        def load_ssp(ssp):\n            input_dask, output_dask = [], []\n            for var in self.input_vars:\n                da_var = ds[var].sel(ssp=ssp)\n                if \"latitude\" in da_var.dims:\n                    da_var = da_var.rename({\"latitude\": \"y\", \"longitude\": \"x\"})\n                if \"member_id\" in da_var.dims:\n                    da_var = da_var.sel(member_id=self.target_member_id)\n                if set(da_var.dims) == {\"time\"}:\n                    da_var = da_var.broadcast_like(spatial_template).transpose(\"time\", \"y\", \"x\")\n                input_dask.append(da_var.data)\n\n            for var in self.output_vars:\n                da_out = ds[var].sel(ssp=ssp, member_id=self.target_member_id)\n                if \"latitude\" in da_out.dims:\n                    da_out = da_out.rename({\"latitude\": \"y\", \"longitude\": \"x\"})\n                output_dask.append(da_out.data)\n\n            return da.stack(input_dask, axis=1), da.stack(output_dask, axis=1)\n\n        train_input, train_output, val_input, val_output = [], [], None, None\n\n        for ssp in self.train_ssps:\n            x, y = load_ssp(ssp)\n            if ssp == \"ssp370\":\n                val_input = x[-self.val_months:]\n                val_output = y[-self.val_months:]\n                train_input.append(x[:-self.val_months])\n                train_output.append(y[:-self.val_months])\n            else:\n                train_input.append(x)\n                train_output.append(y)\n\n        train_input = da.concatenate(train_input, axis=0)\n        train_output = da.concatenate(train_output, axis=0)\n\n        self.normalizer.set_input_statistics(\n            mean=da.nanmean(train_input, axis=(0, 2, 3), keepdims=True).compute(),\n            std=da.nanstd(train_input, axis=(0, 2, 3), keepdims=True).compute(),\n        )\n        self.normalizer.set_output_statistics(\n            mean=da.nanmean(train_output, axis=(0, 2, 3), keepdims=True).compute(),\n            std=da.nanstd(train_output, axis=(0, 2, 3), keepdims=True).compute(),\n        )\n\n        train_input_norm = self.normalizer.normalize(train_input, \"input\")\n        train_output_norm = self.normalizer.normalize(train_output, \"output\")\n        val_input_norm = self.normalizer.normalize(val_input, \"input\")\n        val_output_norm = self.normalizer.normalize(val_output, \"output\")\n\n        test_input, test_output = load_ssp(self.test_ssp)\n        test_input = test_input[-self.test_months:]\n        test_output = test_output[-self.test_months:]\n        test_input_norm = self.normalizer.normalize(test_input, \"input\")\n\n        self.train_dataset = ClimateDataset(train_input_norm, train_output_norm)\n        self.val_dataset = ClimateDataset(val_input_norm, val_output_norm)\n        self.test_dataset = ClimateDataset(test_input_norm, test_output, output_is_normalized=False)\n\n        self.lat = spatial_template.y.values\n        self.lon = spatial_template.x.values\n        self.area_weights = xr.DataArray(get_lat_weights(self.lat), dims=[\"y\"], coords={\"y\": self.lat})\n\n    def train_dataloader(self):\n        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True,\n                          num_workers=self.num_workers, pin_memory=True)\n\n    def val_dataloader(self):\n        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False,\n                          num_workers=self.num_workers, pin_memory=True)\n\n    def test_dataloader(self):\n        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False,\n                          num_workers=self.num_workers, pin_memory=True)\n\n    def get_lat_weights(self):\n        return self.area_weights\n\n    def get_coords(self):\n        return self.lat, self.lon","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T02:11:40.139625Z","iopub.execute_input":"2025-06-01T02:11:40.139872Z","iopub.status.idle":"2025-06-01T02:11:40.156016Z","shell.execute_reply.started":"2025-06-01T02:11:40.139854Z","shell.execute_reply":"2025-06-01T02:11:40.155257Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### üåç Data Module: Loading, Normalization, and Splitting\n\nThis section handles the entire data pipeline, from loading the `.zarr` dataset to preparing PyTorch-ready DataLoaders.\n\n#### `ClimateDataset`\n- A simple PyTorch `Dataset` wrapper that preloads the entire (normalized) dataset into memory using Dask.\n- Converts the data to PyTorch tensors and handles any `NaN` checks up front.\n\n#### `ClimateDataModule`\nA PyTorch Lightning `DataModule` that handles:\n- ‚úÖ **Loading data** from different SSP scenarios and ensemble members\n- ‚úÖ **Broadcasting non-spatial inputs** (like CO‚ÇÇ) to match spatial grid size\n- ‚úÖ **Normalization** using mean/std computed from training data only\n- ‚úÖ **Splitting** into training, validation, and test sets:\n  - Training: All months from selected SSPs (except last 10 years of SSP370)\n  - Validation: Last 10 years (120 months) of SSP370\n  - Test: Last 10 years of SSP245 (unseen scenario)\n- ‚úÖ **Batching** and parallelized data loading via PyTorch `DataLoader`s\n- ‚úÖ **Latitude-based area weighting** for fair climate metric evaluation\n- Shape of the inputs are Batch_Size X 5 (num_input_variables) X 48 X 72\n- Shape of ouputputs are Batch_Size X 2 (num_output_variables) X 48 X 72\n\n> ‚ÑπÔ∏è **Note:** You likely won‚Äôt need to modify this class but feel free to make modifications if you want to inlcude different ensemble mebers to feed more data to your models\n","metadata":{}},{"cell_type":"markdown","source":"### ‚ö° ClimateEmulationModule: Lightning Wrapper for Climate Model Emulation\n\nThis is the core model wrapper built with **PyTorch Lightning**, which organizes the training, validation, and testing logic for the climate emulation task. Lightning abstracts away much of the boilerplate code in PyTorch-based deep learning workflows, making it easier to scale models.\n\n#### ‚úÖ Key Features\n\n- **`training_step` / `validation_step` / `test_step`**: Standard Lightning hooks for computing loss and predictions at each stage. The loss used is **Mean Squared Error (MSE)**.\n\n- **Normalization-aware outputs**:\n  - During validation and testing, predictions and targets are denormalized before evaluation using stored mean/std statistics.\n  - This ensures evaluation is done in real-world units (Kelvin and mm/day).\n\n- **Metric Evaluation** via `_evaluate()`:\n  For each variable (`tas`, `pr`), it calculates:\n  - **Monthly Area-Weighted RMSE**\n  - **Time-Mean RMSE** (RMSE on 10-year average's)\n  - **Time-Stddev MAE** (MAE on 10-year standard deviation; a measure of temporal variability)\n    \n  These metrics reflect the competition's evaluation criteria and are logged and printed.\n\n- **Kaggle Submission Writer**:\n  After testing, predictions are saved to a `.csv` file in the required Kaggle format via `_save_submission()`.\n\n- **Saving Predictions for Visualization**:\n  - Validation predictions are saved tao `val_preds.npy` and `val_trues.npy`\n  - These can be loaded later for visual inspection of the model's performance.\n\n üîß **Feel free to modify any part of this module** (loss functions, evaluation, training logic) to better suit your model or training pipeline / Use pure PyTorch etc.\n\n‚ö†Ô∏è The **final submission `.csv` file must strictly follow the format and naming convention used in `_save_submission()`**, as these `ID`s are used to match predictions to the hidden test set during evaluation.\n\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nclass ClimateEmulationModule(pl.LightningModule):\n    def __init__(self, model, learning_rate=1e-4):\n        super().__init__()\n        self.model = model\n        self.save_hyperparameters(ignore=['model']) # Save all hyperparameters except the model to self.hparams.<param_name>\n        self.criterion = nn.MSELoss()\n        self.normalizer = None\n        self.val_preds, self.val_targets = [], []\n        self.test_preds, self.test_targets = [], []\n        self.train_losses = []\n        self.val_losses = []\n\n    def forward(self, x):\n        return self.model(x)\n\n    def on_fit_start(self):\n        self.normalizer = self.trainer.datamodule.normalizer  # Get the normalizer from the datamodule (see above)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch # Unpack inputs and targets (this is the output of the _getitem_ method in the Dataset above)\n        y_hat = self(x)   # Forward pass\n        loss = self.criterion(y_hat, y)  # Calculate loss\n        self.log(\"train/loss\", loss, prog_bar=True)  # Log loss for tracking\n        return loss\n\n    def on_train_epoch_end(self):\n        loss = self.trainer.callback_metrics.get(\"train/loss\")\n        if loss is not None:\n            self.train_losses.append(loss.item())\n            print(f\"[Epoch {self.current_epoch}] Train Loss: {loss.item():.4f}\")\n            print(f\"[Epoch {self.current_epoch}] Current LR: {self.trainer.optimizers[0].param_groups[0]['lr']:.6f}\")\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = self.criterion(y_hat, y)\n        self.log(\"val/loss\", loss, prog_bar=True)\n\n        y_hat_np = self.normalizer.inverse_transform_output(y_hat.detach().cpu().numpy())\n        y_np = self.normalizer.inverse_transform_output(y.detach().cpu().numpy())\n        self.val_preds.append(y_hat_np)\n        self.val_targets.append(y_np)\n\n        return loss\n\n    def on_validation_epoch_end(self):\n        val_loss = self.trainer.callback_metrics.get(\"val/loss\")\n        if val_loss is not None:\n            self.val_losses.append(val_loss.item())\n            print(f\"[Epoch {self.current_epoch}] Val Loss: {val_loss.item():.4f}\")\n        \n        # Concatenate all predictions and ground truths from each val step/batch into one array\n        preds = np.concatenate(self.val_preds, axis=0)\n        trues = np.concatenate(self.val_targets, axis=0)\n        self._evaluate(preds, trues, phase=\"val\")\n        np.save(\"val_preds.npy\", preds)\n        np.save(\"val_trues.npy\", trues)\n        self.val_preds.clear()\n        self.val_targets.clear()\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        y_hat_np = self.normalizer.inverse_transform_output(y_hat.detach().cpu().numpy())\n        y_np = y.detach().cpu().numpy()\n        self.test_preds.append(y_hat_np)\n        self.test_targets.append(y_np)\n\n    def on_test_epoch_end(self):\n        # Concatenate all predictions and ground truths from each test step/batch into one array\n        preds = np.concatenate(self.test_preds, axis=0)\n        trues = np.concatenate(self.test_targets, axis=0)\n        self._evaluate(preds, trues, phase=\"test\")\n        self._save_submission(preds)\n        self.test_preds.clear()\n        self.test_targets.clear()\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate, weight_decay=1e-5)\n\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, threshold=1e-3, min_lr=1e-7, verbose=True)\n\n        return {\"optimizer\": optimizer, \"lr_scheduler\": {\"scheduler\": scheduler,\"monitor\": \"val/loss\",\"interval\": \"epoch\",\"frequency\": 1}}\n        # return optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n\n    def _evaluate(self, preds, trues, phase=\"val\"):\n        datamodule = self.trainer.datamodule\n        area_weights = datamodule.get_lat_weights()\n        lat, lon = datamodule.get_coords()\n        time = np.arange(preds.shape[0])\n        output_vars = datamodule.output_vars\n\n        for i, var in enumerate(output_vars):\n            p = preds[:, i]\n            t = trues[:, i]\n            p_xr = xr.DataArray(p, dims=[\"time\", \"y\", \"x\"], coords={\"time\": time, \"y\": lat, \"x\": lon})\n            t_xr = xr.DataArray(t, dims=[\"time\", \"y\", \"x\"], coords={\"time\": time, \"y\": lat, \"x\": lon})\n\n            # RMSE\n            rmse = np.sqrt(((p_xr - t_xr) ** 2).weighted(area_weights).mean((\"time\", \"y\", \"x\")).item())\n            # RMSE of time-mean\n            mean_rmse = np.sqrt(((p_xr.mean(\"time\") - t_xr.mean(\"time\")) ** 2).weighted(area_weights).mean((\"y\", \"x\")).item())\n            # MAE of time-stddev\n            std_mae = np.abs(p_xr.std(\"time\") - t_xr.std(\"time\")).weighted(area_weights).mean((\"y\", \"x\")).item()\n\n            print(f\"[{phase.upper()}] {var}: RMSE={rmse:.4f}, Time-Mean RMSE={mean_rmse:.4f}, Time-Stddev MAE={std_mae:.4f}\")\n            self.log_dict({\n                f\"{phase}/{var}/rmse\": rmse,\n                f\"{phase}/{var}/time_mean_rmse\": mean_rmse,\n                f\"{phase}/{var}/time_std_mae\": std_mae,\n            })\n\n    def _save_submission(self, predictions):\n        datamodule = self.trainer.datamodule\n        lat, lon = datamodule.get_coords()\n        output_vars = datamodule.output_vars\n        time = np.arange(predictions.shape[0])\n\n        rows = []\n        for t_idx, t in enumerate(time):\n            for var_idx, var in enumerate(output_vars):\n                for y_idx, y in enumerate(lat):\n                    for x_idx, x in enumerate(lon):\n                        row_id = f\"t{t_idx:03d}_{var}_{y:.2f}_{x:.2f}\"\n                        pred = predictions[t_idx, var_idx, y_idx, x_idx]\n                        rows.append({\"ID\": row_id, \"Prediction\": pred})\n\n        df = pd.DataFrame(rows)\n        filename = f\"kaggle_submission_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n        filepath = os.path.join(\"/kaggle/working\", filename)\n        # os.makedirs(\"submissions\", exist_ok=True)\n        # filepath = f\"submissions/kaggle_submission_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n        df.to_csv(filepath, index=False)\n        print(f\"‚úÖ Submission saved to: {filepath}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T02:11:40.156761Z","iopub.execute_input":"2025-06-01T02:11:40.157018Z","iopub.status.idle":"2025-06-01T02:11:40.174320Z","shell.execute_reply.started":"2025-06-01T02:11:40.156986Z","shell.execute_reply":"2025-06-01T02:11:40.173544Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ‚ö° Training & Evaluation with PyTorch Lightning\n\nThis block sets up and runs the training and testing pipeline using **PyTorch Lightning‚Äôs `Trainer`**, which abstracts away much of the boilerplate in deep learning workflows.\n\n- **Modular Setup**:\n  - `datamodule`: Handles loading, normalization, and batching of climate data.\n  - `model`: A convolutional neural network that maps climate forcings to predicted outputs.\n  - `lightning_module`: Wraps the model with training/validation/test logic and metric evaluation.\n\n- **Trainer Flexibility**:\n  The `Trainer` accepts a wide range of configuration options from `config[\"trainer\"]`, including:\n  - Number of epochs\n  - Precision (e.g., 16-bit or 32-bit)\n  - Device configuration (CPU, GPU, or TPU)\n  - Determinism, logging, callbacks, and more","metadata":{}},{"cell_type":"code","source":"def plot_loss_curves(train_losses, val_losses):\n    plt.figure(figsize=(10, 6))\n    plt.plot(train_losses, label=\"Training Loss\")\n    plt.plot(val_losses, label=\"Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Training & Validation Loss\")\n    plt.legend()\n    plt.grid(True)\n    plt.tight_layout()\n    # plt.savefig(\"loss_curves.png\")\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T02:11:40.175165Z","iopub.execute_input":"2025-06-01T02:11:40.175452Z","iopub.status.idle":"2025-06-01T02:11:40.188760Z","shell.execute_reply.started":"2025-06-01T02:11:40.175413Z","shell.execute_reply":"2025-06-01T02:11:40.188074Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"datamodule = ClimateDataModule(**config[\"data\"])\n\nspecificModel = config[\"model\"][\"type\"]\n\nn_input_channels=len(config[\"data\"][\"input_vars\"])\nn_output_channels=len(config[\"data\"][\"output_vars\"])\n\n\n# model = SimpleCNN(\n#     n_input_channels=len(config[\"data\"][\"input_vars\"]),\n#     n_output_channels=len(config[\"data\"][\"output_vars\"]),\n#     **{k: v for k, v in config[\"model\"].items() if k != \"type\"}\n# )\n\nif(specificModel == \"simple_cnn\"):\n    model = SimpleCNN(n_input_channels, n_output_channels, **{k: v for k, v in config[\"model\"].items() if k != \"type\"})\nelif(specificModel == \"unet\"):\n    model = UNet(n_input_channels, n_output_channels, **{k: v for k, v in config[\"model\"].items() if k != \"type\"})\nelif(specificModel == \"unetAttention\"):\n    model = UNetAttention(n_input_channels, n_output_channels, **{k: v for k, v in config[\"model\"].items() if k not in [\"type\", \"dropout_rate\"]})\nelif(specificModel == \"resnet\"):\n    model = ResNet(n_input_channels, n_output_channels, **{k: v for k, v in config[\"model\"].items() if k != \"type\"})\nelif(specificModel == \"densenet\"):\n    model = DenseNet(n_input_channels, n_output_channels, **{k: v for k, v in config[\"model\"].items() if k != \"type\"})\n\nlightning_module = ClimateEmulationModule(model, learning_rate=config[\"training\"][\"lr\"])\n\nearlyStop = EarlyStopping(\n    monitor=\"val/loss\",\n    patience=15,\n    mode=\"min\",\n    verbose=True\n)\n\ngoodModelCheckpoint = ModelCheckpoint(\n    dirpath=\"/kaggle/working/checkpoints\",\n    monitor=\"val/loss\",\n    mode=\"min\",\n    save_top_k=1,\n    filename=\"best-checkpoint\",\n)\n\ntrainer = pl.Trainer(\n    callbacks=[earlyStop, goodModelCheckpoint],\n    **config[\"trainer\"]\n)\n\n# trainer = pl.Trainer(**config[\"trainer\"])\ntrainer.fit(lightning_module, datamodule=datamodule)   # Training\n\nplot_loss_curves(lightning_module.train_losses, lightning_module.val_losses)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T02:11:40.189637Z","iopub.execute_input":"2025-06-01T02:11:40.189966Z","iopub.status.idle":"2025-06-01T03:10:07.621832Z","shell.execute_reply.started":"2025-06-01T02:11:40.189938Z","shell.execute_reply":"2025-06-01T03:10:07.620907Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Test model\n\n**IMPORTANT:** Please note that the test metrics will be bad because the test targets have been corrupted on the public Kaggle dataset.\nThe purpose of testing below is to generate the Kaggle submission file based on your model's predictions, which you can submit to the competition.","metadata":{}},{"cell_type":"code","source":"best_model_path = goodModelCheckpoint.best_model_path\nbest_model = ClimateEmulationModule.load_from_checkpoint(\n    best_model_path,\n    model=model,  # Pass your model architecture again\n    learning_rate=config[\"training\"][\"lr\"],\n)\n\nbest_model.normalizer = datamodule.normalizer\n\n# Evaluate using the best model\ntrainer.test(best_model, datamodule=datamodule)\n# trainer.test(lightning_module, datamodule=datamodule) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T03:10:07.622860Z","iopub.execute_input":"2025-06-01T03:10:07.623166Z","iopub.status.idle":"2025-06-01T03:10:38.095220Z","shell.execute_reply.started":"2025-06-01T03:10:07.623140Z","shell.execute_reply":"2025-06-01T03:10:38.094365Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Plotting Utils\n","metadata":{}},{"cell_type":"code","source":"def plot_comparison(true_xr, pred_xr, title, cmap='viridis', diff_cmap='RdBu_r', metric=None):\n    fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n\n    vmin = min(true_xr.min().item(), pred_xr.min().item())\n    vmax = max(true_xr.max().item(), pred_xr.max().item())\n\n    # Ground truth\n    true_xr.plot(ax=axs[0], cmap=cmap, vmin=vmin, vmax=vmax, add_colorbar=True)\n    axs[0].set_title(f\"{title} (Ground Truth)\")\n\n    # Prediction\n    pred_xr.plot(ax=axs[1], cmap=cmap, vmin=vmin, vmax=vmax, add_colorbar=True)\n    axs[1].set_title(f\"{title} (Prediction)\")\n\n    # Difference\n    diff = pred_xr - true_xr\n    abs_max = np.max(np.abs(diff))\n    diff.plot(ax=axs[2], cmap=diff_cmap, vmin=-abs_max, vmax=abs_max, add_colorbar=True)\n    axs[2].set_title(f\"{title} (Difference) {f'- {metric:.4f}' if metric else ''}\")\n\n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T03:10:38.096361Z","iopub.execute_input":"2025-06-01T03:10:38.096765Z","iopub.status.idle":"2025-06-01T03:10:38.103153Z","shell.execute_reply.started":"2025-06-01T03:10:38.096726Z","shell.execute_reply":"2025-06-01T03:10:38.102207Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### üñºÔ∏è Visualizing Validation Predictions\n\nThis cell loads saved validation predictions and compares them to the ground truth using spatial plots. These visualizations help you qualitatively assess your model's performance.\n\nFor each output variable (`tas`, `pr`), we visualize:\n\n- **üìà Time-Mean Map**: The 10-year average spatial pattern for both prediction and ground truth. Helps identify long-term biases or spatial shifts.\n- **üìä Time-Stddev Map**: Shows the standard deviation across time for each grid cell ‚Äî useful for assessing how well the model captures **temporal variability** at each location.\n- **üïì Random Timestep Sample**: Visual comparison of prediction vs ground truth for a single month. Useful for spotting fine-grained anomalies or errors in specific months.\n\n> These plots provide intuition beyond metrics and are useful for debugging spatial or temporal model failures.\n","metadata":{}},{"cell_type":"code","source":"# Load validation predictions\n# make sure to have run the validation loop at least once\nval_preds = np.load(\"val_preds.npy\")\nval_trues = np.load(\"val_trues.npy\")\n\nlat, lon = datamodule.get_coords()\noutput_vars = datamodule.output_vars\ntime = np.arange(val_preds.shape[0])\n\nfor i, var in enumerate(output_vars):\n    pred_xr = xr.DataArray(val_preds[:, i], dims=[\"time\", \"y\", \"x\"], coords={\"time\": time, \"y\": lat, \"x\": lon})\n    true_xr = xr.DataArray(val_trues[:, i], dims=[\"time\", \"y\", \"x\"], coords={\"time\": time, \"y\": lat, \"x\": lon})\n\n    # --- Time Mean ---\n    plot_comparison(true_xr.mean(\"time\"), pred_xr.mean(\"time\"), f\"{var} Val Time-Mean\")\n\n    # --- Time Stddev ---\n    plot_comparison(true_xr.std(\"time\"), pred_xr.std(\"time\"), f\"{var} Val Time-Stddev\", cmap=\"plasma\")\n\n    # --- Random timestep ---\n    t_idx = np.random.randint(0, len(time))\n    plot_comparison(true_xr.isel(time=t_idx), pred_xr.isel(time=t_idx), f\"{var} Val Sample Timestep {t_idx}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T03:10:38.103941Z","iopub.execute_input":"2025-06-01T03:10:38.104145Z","iopub.status.idle":"2025-06-01T03:10:42.838897Z","shell.execute_reply.started":"2025-06-01T03:10:38.104126Z","shell.execute_reply":"2025-06-01T03:10:42.838055Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load validation predictions\n# make sure to have run the validation loop at least once\nval_preds = np.load(\"val_preds.npy\")\nval_trues = np.load(\"val_trues.npy\")\n\nlat, lon = datamodule.get_coords()\noutput_vars = datamodule.output_vars\ntime = np.arange(val_preds.shape[0])\n\nt_idx = np.random.randint(0, len(time))\nfor i, var in enumerate(output_vars):\n    pred_xr = xr.DataArray(val_preds[:, i], dims=[\"time\", \"y\", \"x\"], coords={\"time\": time, \"y\": lat, \"x\": lon})\n    true_xr = xr.DataArray(val_trues[:, i], dims=[\"time\", \"y\", \"x\"], coords={\"time\": time, \"y\": lat, \"x\": lon})\n    \n    plot_comparison(true_xr.isel(time=t_idx), pred_xr.isel(time=t_idx), f\"{var} Val Sample Timestep {t_idx}\")\n\nt_idx = np.random.randint(0, len(time))\nfor i, var in enumerate(output_vars):\n    pred_xr = xr.DataArray(val_preds[:, i], dims=[\"time\", \"y\", \"x\"], coords={\"time\": time, \"y\": lat, \"x\": lon})\n    true_xr = xr.DataArray(val_trues[:, i], dims=[\"time\", \"y\", \"x\"], coords={\"time\": time, \"y\": lat, \"x\": lon})\n    \n    plot_comparison(true_xr.isel(time=t_idx), pred_xr.isel(time=t_idx), f\"{var} Val Sample Timestep {t_idx}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T03:10:42.839868Z","iopub.execute_input":"2025-06-01T03:10:42.840204Z","iopub.status.idle":"2025-06-01T03:10:46.367179Z","shell.execute_reply.started":"2025-06-01T03:10:42.840173Z","shell.execute_reply":"2025-06-01T03:10:46.366259Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üß™ Final Notes\n\nThis notebook is meant to serve as a **baseline template** ‚Äî a starting point to help you get up and running quickly with the climate emulation challenge.\n\nYou are **not** required to stick to this exact setup. In fact, we **encourage** you to:\n\n- üîÅ Build on top of the provided `DataModule`. \n- üß† Use your own model architectures or training pipelines that you‚Äôre more comfortable with \n- ‚öóÔ∏è Experiment with ideas  \n- ü•á Compete creatively to climb the Kaggle leaderboard  \n- üôå Most importantly: **have fun** and **learn as much as you can** along the way\n\nThis challenge simulates a real-world scientific problem, and there‚Äôs no single \"correct\" approach ‚Äî so be curious, experiment boldly, and make it your own!\n","metadata":{}},{"cell_type":"code","source":"data_path = config[\"data\"][\"path\"]\ndata = xr.open_zarr(data_path)\n\n# Select relevant SSPs and average over ensemble members\n# Set up 2 rows (variables) x 3 columns (SSPs)\n\nssps = [\"ssp126\", \"ssp370\", \"ssp585\"]\nsubset = data.sel(ssp=ssps).mean(dim=[\"member_id\"])\n\ntarget_vars = [\"tas\", \"pr\"]\n\nfig, ax = plt.subplots(len(target_vars), len(ssps), figsize=(18, 12))\n\nfor i, var in enumerate(target_vars):\n    for j, ssp in enumerate(ssps):\n        vals = subset[var].sel(ssp=ssp).values.flatten()\n        ax[i, j].hist(vals, bins=100)\n        ax[i, j].set_title(f\"{var} - {ssp}\")\n        ax[i, j].set_xlabel(f\"{var} value\")\n        ax[i, j].set_ylabel(\"Frequency\")\n\n        mean_val = np.nanmean(vals)\n        std_val = np.nanstd(vals)\n        print(f\"{var} ({ssp}) ‚Üí mean: {mean_val:.2f}, std: {std_val:.2f}\")\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T03:10:46.368157Z","iopub.execute_input":"2025-06-01T03:10:46.368511Z","iopub.status.idle":"2025-06-01T03:10:50.783018Z","shell.execute_reply.started":"2025-06-01T03:10:46.368479Z","shell.execute_reply":"2025-06-01T03:10:50.782100Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ssps = [\"ssp126\", \"ssp370\", \"ssp585\"]\nsubset = data.sel(ssp=ssps).mean(dim=[\"member_id\"])\n\nchosen_vars = [\"CO2\", \"CH4\", \"rsdt\"]\n\nfig, axs = plt.subplots(len(chosen_vars), len(ssps), figsize=(18, 12))\n\nfor i, var in enumerate(chosen_vars):\n    for j, ssp in enumerate(ssps):\n        vals = subset[var].sel(ssp=ssp).values.flatten()\n        axs[i, j].hist(vals, bins=100)\n        axs[i, j].set_title(f\"{var} - {ssp}\")\n        axs[i, j].set_xlabel(f\"{var} value\")\n        axs[i, j].set_ylabel(\"Frequency\")\n\n        mean_val = np.nanmean(vals)\n        std_val = np.nanstd(vals)\n        print(f\"{var} ({ssp}) ‚Üí mean: {mean_val:.2f}, std: {std_val:.2f}\")\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T03:10:50.785711Z","iopub.execute_input":"2025-06-01T03:10:50.785935Z","iopub.status.idle":"2025-06-01T03:10:54.561417Z","shell.execute_reply.started":"2025-06-01T03:10:50.785915Z","shell.execute_reply":"2025-06-01T03:10:54.560645Z"}},"outputs":[],"execution_count":null}]}